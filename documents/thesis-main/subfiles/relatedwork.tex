% comment out one of the next two lines accordingly
\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Related Work}\label{rel:chap}

\section{Natural Language and Sequential Decision Making}

Fueled by the Transformer architecture~\citep{vaswani_attention_2017}, advancements in natural
language processing~\citep{vaswani_attention_2017, devlin_bert_2019, brown_language_2020} are now
finding themselves in other domains of ML~\citep{radford_learning_2021,
	rombach_high-resolution_2022, dhariwal_jukebox_2020} as the field becomes increasingly multi-modal~\citep{baltrusaitis_multimodal_2019, xu_multimodal_2022} and the boundaries between paradigms fade.
Moreover, the natural language interface afforded by these models provides a means for leveraging
expressive and flexible user input, leading to advancements in ``prompt engineering'' for in-context
learning (ICL)~\citep{dohan_language_2022, dosovitskiy_image_2022, reynolds_prompt_2021,
	wei_chain--thought_2022, hertz_prompt--prompt_2022}. On this note, recent work in SDM has also
explored the integration of natural language in existing or new SDM methods~\citep{luketina_survey_2019} and (more famously) viceversa~\citep{ouyang_training_2022}. This work
focuses on the former case. Here, Google's SayCan~\citep{ahn_as_2022} and DeepMind's Gato~\citep{reed_generalist_2022} are the current reference works, the first utilizing language models
for planning robotics tasks and the latter leveraging a multimodal transformer to perform
decision-making aided by language and other modalities. \Citet{jiang_vima_2022}'s VIMA presents
a similar approach to Gato, devoting more attention to the design of multimodal prompts. Moving away
from purely transformer-based solutions, CLIPort~\citep{shridhar_cliport_2021} use CLIP~\citep{radford_learning_2021} and Transporter~\citep{zeng_transporter_2021} in a two-stream
architecture for language-specified manipulation tasks using imitation learning. More distantly,
\citet{zhou_inverse_2021} attempt to address generalization issues when using NL goals by proposing
a sample-efficient inverse reinforcement learning algorithm based on latent goal relabeling~\citep{nair_visual_2018}. \Citet{choi_lmpriors_2022} also tackle generalization through their
LMPriors framework which shapes the reward~\citep{ng_policy_1999} based on priors computed by
a generative language model. Instead of providing input instructions or feedback,
\citet{lampinen_tell_2022} take a different approach by making their agents produce NL explanations
as part of their output, which is then used in an additional loss term for learning causal structure
and OOD generalization. \Citet{yang_safe_2021} focus on developing an operationally safe RL
algorithm by proposing a modular \textit{constraint interpreter} capable of mapping NL constraints
to spatial and temporal representations of hidden states. \Citet{deepmind_creating_2022} and
\citet{fan_minedojo_2022} leverage contrastive vision-language training to address the issue of
grounding language representations to the environment, while \citet{watkins_teachable_2021} proposes
a novel bootstrapping solution for grounding. \Citet{sumers_learning_2021} explores reward learning
using open-ended linguistic feedback. The same authors contribute formalizations of the type of
language a modeled speaker could use for preference expression~\citep{sumers_how_2022} and most
recently by extending hindsight experience replay (HER)~\citep{andrychowicz_hindsight_2017} to
a language-conditioned setting using generative visual-language models (VLM)~\citep{alayrac_flamingo_2022}. A number of language-annotated datasets are available for research in
this area~\citep{zholus_iglu_2022, mees_calvin_2022, fan_minedojo_2022, shridhar_alfred_2020,
	jiang_yunfan_vima_2022, liu_reinforcement_2022}. However the field is still clearly in early stages
and has yet to settle on a particular direction. To our knowledge, this work is the first to
investigate the effects of NL on goal misgeneralization in SDM.

\section{Causal Confusion and Goal Misgeneralization}

The issue of causal confusion was first identified and defined by \citet{de_haan_causal_2019} in the
context of imitation learning. They address the issue by learning a graph-parametrized policy for
each possible causal graph and subsequently performing targeted interventions to select the best
policy. \Citet{tien_study_2022} later successfully identify the same phenomenon in the context of
preference-based~\citep{christiano_deep_2017} inverse reinforcement learning
(IRL)~\citep{ng_algorithms_2000}. Concurrently, \citet{gupta_can_2022} identify causal confusion in
the more relevant context of offline RL, and explore active sampling as a means to mitigate the
issue. While these works focus mainly on capability failures, \citet{kirk_causal_2022} also
recognize goal misgeneralization and incentive mismanagement~\citep{farquhar_path-specific_2022} as
two additional failure modes, where we focus on the former of the two. \Citet{langosco_goal_2022}
are the first to formally define goal misgeneralization based on \citet{orseau_agents_2018}'s
definition of agency. The authors demonstrate the phenomenon in a number of RL agents trained on the
Procgen~\citep{cobbe_leveraging_2020} benchmark, proposing increased training data diversity as
a means of alleviating the issue. \Citet{shah_goal_2022} later generalise the definition, removing
the assumed RL framework necessary in \citet{langosco_goal_2022}'s formalization, and demonstrating
the issue in a variety of new settings. Aside from more diverse training data, the authors suggest
uncertainty-aware models, better inductive biases and techniques targeting deception as potential
routes for mitigation. The issue of goal misgeneralization draws parallels with \textit{reward
	hacking}~\citep{pan_effects_2022, skalse_defining_2022} and \textit{reward
	tampering}~\citep{everitt_reward_2021} where there is a misalignment between the designers intended
behaviour and the algorithm's behaviour due to misspecified rewards. This misalignment makes goal
misgeneralization of particular interest to research in Artificial Intelligence (AI)
alignment~\citep{ngo_alignment_2022} and AI safety more broadly~\citep{hendrycks_unsolved_2022,
	houben_inspect_2022}.

\section{Representation Learning and Foundation Models}

Our approach of working on the representations produced by a pre-trained multimodal model such as
CLIP~\citep{radford_learning_2021} is closely related to the field of \emph{representation
learning}. This is the study of the processes for learning transformations of raw input data into
more abstract representations, typically in the form of dense vectors. The idea is that the more
abstract nature of the learned features can lead to better generalization, for usefulness and
applications on a wide range of downstream tasks, omitting the need for less transportable
hand-engineered features. Now a ubiquitous component of modern-day ML, in
2013~\citeauthor{bengio_representation_2013}~identify the paradigm-shift from feature engineering to
representation learning. The authors define desiderata for ``good representations'', such as local
smoothness, hierarchically-organised explanatory factors shared across tasks and a sparse activation
for a specific input. They compare representation learning in the context of probabilistic graphical
models (PGM) and neural networks and outline connections to manifold learning and
invariance~\citep{bronstein_geometric_2021}. More recently,~\citet{liu_representation_2020} write
a book covering representation learning in NLP, while~\citet{xie_representation_2020} provide
a statistical perspective, linking representation learning to factor analysis~\citep{rubin_em_1982}
and multidimensional scaling~\citep{kruskal_multidimensional_1964}. Readers of our work may find the
ideas of \citet{scholkopf_toward_2021} of particular interest. Here, the authors relate fundamental
concepts of causal inference to representation learning, defining \emph{causal representation
learning} as the problem of discovering high-level causal variables from low-level observations.
\citet{lopez-paz_discovering_2017} work precisely on this issue of causal discovery in the context
of objects in images, leveraging proxy variables for identifying causal relationships between
entities in images. In the context of SDM,~\citet{lan_generalization_2022} study the generalization
of state representations in RL, relying on the notion of ``effective dimension''. They provide
a bound on the generalization error that arises when using a given $k$-dimensional representation.
They demonstrate the usefulness of the bound in the context of successor representations~\citep{dayan_improving_1993}. Closely related to CLIP,~\citet{guo_deep_2019}
and~\citet{le-khac_contrastive_2020} provide surveys covering multimodal and contrastive
representation learning respectively. The latter builds on the aforementioned work
from~\citet{bengio_representation_2013} and identify core principals for learning good
representations: distributedness, abstraction, invariance and disentanglement. Much modern work in
the field points to foundation models and the use of their representations for downstream tasks~\citep{bommasani_opportunities_2021, zhou_comprehensive_2023}. While originating mostly in the field
of NLP~\citep{devlin_bert_2019, brown_language_2020, touvron_llama_2023}, attention has increasingly
included or shifted to other modalities. Here, CLIP~\citep{radford_learning_2021} is the most
prominent example. Leveraging a carefully curated dataset of image-caption pairs and a contrastive
loss, CLIP learns semantically similar representations across text and vision symbols. Similar to
CLIP, ALIGN~\citep{jia_scaling_2021} relaxes the need for careful curation of the dataset,
instead relying on a noisier but larger dataset of images and alt-text descriptions. They
demonstrate that expert knowledge is not necessary for dataset curation in contrastive learning.
BEIT-3~\citep{wang_image_2022} and M3AE~\citep{geng_multimodal_2022} take a different approach:
instead of training an encoder for each modality and ensuring that the representations from each are
similar, they train a single encoder capable of handling both modalities, avoiding the need for
paired data and hence enabling larger scale training. FLAVA~\citep{singh_flava_2022} operates in
a similar vein, utilizing a single model for multimodal representations. It however also includes
cross-modal contrastive objectives, aiming for performance on image-only, text-only and image-text
downstream tasks with the same single model. Florence~\citep{yuan_florence_2021} expands the
dimensions covered by their representations, covering for instance both static (images) and dynamic
(videos) inputs, or coarse (scene) and fine-grained (object) tasks.

\section{Grounding}

The work by \citet{harnad_symbol_1990} typically acts as the starting reference for most work on the
symbol grounding problem. In machine learning and linguistics, the focus is typically on grounding
in the context of of natural language understanding (NLU) and meaning~\citep{winograd_understanding_1972}. \Citet{clark_grounding_1991} and \citet{devault_societal_2006}
are examples of early work in this theme. A commonly held perspective is that referents from other
modalities are a necessary ingredient for grounding language. \Citet{mooney_learning_2008} make
early connections to perception of visual symbols as a means of grounding natural language for NLU.
Contemporary to their influential work on distributional semantics~\citep{baroni_dont_2014,
baroni_frege_2014}, \citet{bruni_multimodal_2014} propose \emph{multimodal} distributional semantics
to address the lack of perceptual grounding of distributional models. In particular, they leverage
computer vision techniques to extract ``visual words'' from images and incorporate these into the
training data such that the distributional representations can capture these references. More
recently, some focus has been dedicated to comparing how grounded language and meaning is learned in
humans and in machines~\citep{lake_word_2023}. \Citet{linzen_how_2020} touch on the subject in their
position paper critiquing the current paradigm for NLU evaluation. They argue, among other things,
that humans do not learn language from text alone, but additionally through their experience of the
world, and that more careful attention should be dedicated to this direction. In parallel,
\citet{bisk_experience_2020} make similar claims around the usefulness of world experience for
linguistic grounding and NLU. They propose the notion of ``World Scopes (WS)'' as a framework for
auditing progress in NLP. \Citet{bender_climbing_2020} outline how current state-of-the-art
techniques are limited in their ability to acquire meaning, due to the lack of grounding referents.
\Citet{piantadosi_meaning_2022} directly respond to this critique, offering an alternative
explanation for how grounding and meaning can be achieved through the lens of conceptual role
theory~\citep{block_conceptual_1998}.


\ifSubfilesClassLoaded{%
	\bibliographystyle{\subfix{bibstyle}}
	\bibliography{\subfix{references-bibtex}}%
}{}
\end{document}
