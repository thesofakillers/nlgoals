% comment out one of the next two lines accordingly
\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Offline Reinforcement Learning}

Research on Offline RL can be traced back to the work of \citet{ernst_tree-based_2005} on ``batch
reinforcement learning'', approximating the Q-function using an ensemble of tree-based supervised
learning methods. Contemporary work from \citet{riedmiller_neural_2005} proposes NFQ, a neural
counterpart. \Citet{lange_batch_2012} present a first tutorial on the field, but the more modern
incarnation is mostly spearheaded by \citet{kalashnikov_scalable_2018}, which explored utilizing
data collected from an ensemble of robots to train a single a new Q-function from scratch without
further interactions with the environment. Soon after, \citet{fujimoto_off-policy_2019} address the
limitations with Batch-Constrained deep Q-learning (BCQ), constraining the learned policy to choose
state-actions pairs that are close to those contained in the offline dataset, inspiring further
constraint-based Offline RL methods \citep{kumar_stabilizing_2019, xu_offline_2021}. Contrastingly,
instead of constraining the policy, \citet{kumar_conservative_2020} propose conservative Q-learning
(CQL) which regularizes the Q-function so that out-of-distribution state-action pairs are assigned
lower values. \Citet{yu_combo_2021} develop a practical model-based CQL variant, while
\citet{singh_cog_2020} successfully demonstrate that CQL-based offline RL can leverage a large and
diverse prior unlabelled dataset for performance on a smaller downstream supervised task. Work in
offline RL is typically evaluated on the D4RL \citep{fu_d4rl_2021} and RL Unplugged
\citep{gulcehre_rl_2020} benchmarks. For further information, readers are directed to the most
recently published surveys of the field \citep{levine_offline_2020, prudencio_survey_2022}.

\section{Causal Confusion and Goal Misgeneralization} 

The issue of causal confusion was first identified and defined by \citet{de_haan_causal_2019} in the
context of imitation learning. They address the issue by learning a graph-parametrized policy for
each possible causal graph and subsequently performing targeted interventions to select the best
policy. \Citet{tien_study_2022} later successfully identify the same phenomenon in the context of
preference-based \citep{christiano_deep_2017} inverse reinforcement learning (IRL)
\citep{ng_algorithms_2000}. Concurrently, \citet{gupta_can_2022} identify causal confusion in the
more relevant context of offline RL, and explore active sampling as a means to mitigate the issue.
While these works focus mainly on capability failures, \citet{kirk_causal_2022} also recognize goal
misgeneralization and incentive mismanagement \citep{farquhar_path-specific_2022} as two additional
failure modes, where we focus on the former of the two. \Citet{langosco_goal_2022} are the first to
formally define goal misgeneralization based on \citet{orseau_agents_2018}'s definition of agency.
The authors demonstrate the phenomenon in a number of RL agents trained on the Procgen
\citep{cobbe_leveraging_2020} benchmark, proposing increased training data diversity as a means of
alleviating the issue. \Citet{shah_goal_2022} later generalise the definition, removing the assumed
RL framework necessary in \citet{langosco_goal_2022}'s formalization, and demonstrating the issue in
a variety of new settings. Aside from more diverse training data, the authors suggest
uncertainty-aware models, better inductive biases and techniques targeting deception as potential
routes for mitigation. The issue of goal misgeneralization draws parallels with \textit{reward
hacking} \citep{pan_effects_2022, skalse_defining_2022} and \textit{reward tampering}
\citep{everitt_reward_2021} where there is a misalignment between the designers intended behaviour
and the algorithm's behaviour due to misspecified rewards. This misalignment makes goal
misgeneralization of particular interest to research in Artificial Intelligence (AI) alignment
\citep{ngo_alignment_2022} and AI safety more broadly \citep{hendrycks_unsolved_2022,
houben_inspect_2022}.

\section{Natural Language and Reinforcement Learning} 

Fueled by the Transformer architecture \citep{vaswani_attention_2017}, advancements in natural
language processing \citep{vaswani_attention_2017, devlin_bert_2019, brown_language_2020} are now
finding themselves in other domains of ML \citep{radford_learning_2021,
rombach_high-resolution_2022, dhariwal_jukebox_2020} as the field becomes increasingly multi-modal
\citep{baltrusaitis_multimodal_2019, xu_multimodal_2022} and the boundaries between paradigms fade.
Moreover, the natural language interface afforded by these models provides a means for leveraging
expressive and flexible user input, leading to advancements in ``prompt engineering'' for in-context
learning (ICL) \citep{dohan_language_2022, dosovitskiy_image_2022, reynolds_prompt_2021,
wei_chain--thought_2022, hertz_prompt--prompt_2022}. On this note, recent work in RL has also
explored the integration of natural language in existing or new RL methods
\citep{luketina_survey_2019} and (more famously) viceversa \citep{ouyang_training_2022}. This work
focuses on the former case. Here, Google's SayCan \citep{ahn_as_2022} and DeepMind's Gato
\citep{reed_generalist_2022} are the current reference works, the first utilizing language models
for planning robotics tasks and the latter leveraging a multimodal transformer to perform
decision-making aided by language and other modalities. \Citet{jiang_vima_2022}'s VIMA presents
a similar approach to Gato, devoting more attention to the design of multimodal prompts. Moving away
from purely transformer-based solutions, CLIPort \citep{shridhar_cliport_2021} use CLIP
\citep{radford_learning_2021} and Transporter \citep{zeng_transporter_2021} in a two-stream
architecture for language-specified manipulation tasks using imitation learning. More distantly,
\citet{zhou_inverse_2021} attempt to address generalization issues when using NL goals by proposing
a sample-efficient inverse reinforcement learning algorithm based on latent goal relabeling
\citep{nair_visual_2018}. \Citet{choi_lmpriors_2022} also tackle generalization through their
LMPriors framework which shapes the reward \citep{ng_policy_1999} based on priors computed by
a generative language model. Instead of providing input instructions or feedback,
\citet{lampinen_tell_2022} take a different approach by making their agents produce NL explanations
as part of their output, which is then used in an additional loss term for learning causal structure
and OOD generalization. \Citet{yang_safe_2021} focus on developing an operationally safe RL
algorithm by proposing a modular \textit{constraint interpreter} capable of mapping NL constraints
to spatial and temporal representations of hidden states. \Citet{deepmind_MIA_2022} and
\citet{fan_minedojo_2022} leverage contrastive vision-language training to address the issue of
grounding language representations to the environment, while \citet{watkins_teachable_2021} proposes
a novel bootstrapping solution for grounding. \Citet{sumers_learning_2021} explores reward learning
using open-ended linguistic feedback. The same authors contribute formalizations of the type of
language a modeled speaker could use for preference expression \citep{sumers_how_2022} and most
recently by extending hindsight experience replay (HER) \citep{andrychowicz_hindsight_2017} to
a language-conditioned setting using generative visual-language models (VLM)
\citep{alayrac_flamingo_2022}. A number of language-annotated datasets are available for research in
this area \citep{zholus_iglu_2022, mees_calvin_2022, fan_minedojo_2022, shridhar_alfred_2020,
jiang_yunfan_vima_2022, liu_reinforcement_2022}. However the field is still clearly in early stages
and has yet to settle on a particular direction. To our knowledge, this work is the first to
investigate the effects of NL on goal misgeneralization in offline RL.


\ifSubfilesClassLoaded{%
  \bibliographystyle{../bibstyle}
  \bibliography{../references-bibtex}%
}{}
\end{document}
